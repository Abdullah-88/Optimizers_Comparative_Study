# Conference paper, EAI ROSENET 2023 - 7th EAI International Conference on Robotics and Networks 
# Comparatively Studying Modern Optimizers Capability for Fitting Vision Transformers
Abdullah Nazhat Abdullah, 
Tarkan Aydin

## Abstract
The Transformer architectures have been achieving great strides in both research and industry, garnering high adoption due to
their versatility and generality. These qualities, combined with the availability of internet-scale datasets, open the path to constructing Deep
Learning systems that can target many modalities and several tasks within each modality. Throughout the years, many optimization algorithms have been proposed and utilized in fitting Deep Learning models. Although many comparative assessments were made that investigated
analyzing and selecting the best optimizer to fit architectures prior to Transformers, the literature lacks such extensive assessments in relation
to optimizing Transformer-based Deep Learning models. In this paper, we investigated modern and recently introduced Deep Learning optimizers and applied the comparative assessment to multiple Transformer architectures implemented for the task of image classification. It was
discovered experimentally by our comparative study that the novel optimizer LION provided the best performance on the target task and datasets, proving that the algorithmic design of optimizers can compete with and surpass handcrafted optimization schemes that are normally used in fitting Transformer architectures.

## Getting the code

You can download a copy of all the files in this repository by cloning the
[git](https://git-scm.com/) repository:

    git clone https://github.com/Abdullah-88/Optimizers_Comparative_Study.git

or [download a zip archive](https://github.com/Abdullah-88/Optimizers_Comparative_Study/archive/master.zip).


## License

All source code is made available under a BSD 3-clause license. You can freely
use and modify the code, without warranty, so long as you provide attribution
to the authors. See `LICENSE.md` for the full license text.

The manuscript text is not open source. The authors reserve the rights to the
article content, which is currently submitted for publication.
